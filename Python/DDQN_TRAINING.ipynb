{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4995 [00:00<?, ?episodes/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4995 [00:07<?, ?episodes/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXITED WITH 6th episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from Env import _5G\n",
    "from DDQN_AGENT import *\n",
    "from collections import deque   \n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pickle\n",
    "N= 5    #CUE тоо\n",
    "M= 14   #DTUE тоо\n",
    "MODEL_NAME = 'TEST_NUM03'#сургасан үр дүнгээ хадгалах нэр(log/ дотор хадгалагдана)\n",
    "\n",
    "EPISODES = 5000 #Хэдэн state дээр ажиллахаа энд тохируулна\n",
    "NUM_OF_ITER = 100 #Нэг state дээр хэдэн удаа allocation хийхээ энд тохируулна\n",
    "\n",
    "\n",
    "EPSILON_DECAY = 0.995   #Нэг episode бүрийн эцсийн Decay\n",
    "MIN_EPSILON = 0.01      #Хамгийн бага epsilona\n",
    "Smoothing_memory = 100  #Үр дүгн сүүлийн Smoothing_memory утгаар дундаж(минимум, максимум) утгыг харах\n",
    "\n",
    "\n",
    "\n",
    "MODEL_NAME = 'DDQN_' + MODEL_NAME\n",
    "env = _5G(M,N)\n",
    "agent = DDQN_Agent(M, N, MODEL_NAME)\n",
    "starting_episode = 1\n",
    "min_total= np.inf\n",
    "total_reward= deque(maxlen=Smoothing_memory)\n",
    "total_memory= deque(maxlen=Smoothing_memory)    \n",
    "epsilon = 1\n",
    "\n",
    "file_path = 'saved_weights/' + MODEL_NAME + 'primary.index'\n",
    "if os.path.exists(file_path):\n",
    "    agent.model.load_weights('saved_weights/' + MODEL_NAME + 'primary')\n",
    "    agent.target_model.load_weights('saved_weights/' + MODEL_NAME + 'target')\n",
    "    with open('saved_weights/step_' + MODEL_NAME + '.txt', 'rb') as file:\n",
    "        f = pickle.load(file)\n",
    "        starting_episode = f[1]\n",
    "        step = f[0]\n",
    "        agent.replay_memory = deque(f[2], maxlen=REPLAY_MEMORY_SIZE)\n",
    "    agent.tensorboard.step = int(step)\n",
    "    with open('saved_weights/total_' + MODEL_NAME + '.txt', 'rb') as file:\n",
    "        total_memory = pickle.load( file)\n",
    "        total_memory = deque(total_memory, maxlen=Smoothing_memory)\n",
    "    with open('saved_weights/reward_' + MODEL_NAME + '.txt', 'rb') as file:\n",
    "        total_reward = pickle.load( file)\n",
    "        total_reward = deque(total_reward, maxlen=Smoothing_memory)\n",
    "\n",
    "try:\n",
    "    for episode in tqdm(range(starting_episode, EPISODES + 1), ascii=False, unit='episodes'):\n",
    "\n",
    "\n",
    "        current_state_ = env.reset()\n",
    "        current_state = current_state_['DRUE'][np.argsort(current_state_['DRUE'][:, 0]), :]/40 + 0.5 \n",
    "        current_state= np.concatenate((current_state, np.array(current_state_['DTUE'][np.argsort(current_state_['DTUE'][:, 0]), :])/1200 + 0.5))\n",
    "        current_state= np.concatenate((current_state, np.array(current_state_['CUE'][np.argsort(current_state_['CUE'][:, 0]), :])/1200 + 0.5))\n",
    "\n",
    "        for _ in range(NUM_OF_ITER):\n",
    "            if np.random.random() > epsilon:\n",
    "                Q = agent.get_qs(current_state)[0]\n",
    "                action = np.array([np.argmax(i) for i in Q.reshape(M,N)])\n",
    "            else:\n",
    "                action = np.random.randint(0, N, size=(M,))\n",
    "            \n",
    "            #FORCED VALID ALLOCATION##############################\n",
    "            #counter = 0\n",
    "            # while not env.step(action)[1] and counter<50:\n",
    "            #     action = np.random.randint(0, N, size=(M,))\n",
    "            #     counter += 1\n",
    "            #################################################\n",
    "\n",
    "            done, tot, std = env.step(action)\n",
    "            reward= 0\n",
    "            #MEASURE REWARD\n",
    "            if tot:\n",
    "                if tot < min_total:\n",
    "                    min_total = tot\n",
    "                total_memory.append(tot)\n",
    "                dr = tot - min_total if len(total_memory) > 1 else 0\n",
    "                reward = dr if dr >= 0 else -std * 2\n",
    "                total_reward.append(reward)\n",
    "            else:\n",
    "                reward = -10\n",
    "                total_reward.append(reward)\n",
    "\n",
    "\n",
    "            if total_memory:\n",
    "                #MONITORING\n",
    "                average_reward = np.mean(total_reward)\n",
    "                min_reward = np.min(total_reward)\n",
    "                max_reward = np.max(total_reward)\n",
    "                min_total = np.min(total_memory)\n",
    "                max_total = np.max(total_memory)\n",
    "                avg_total = np.mean(total_memory)\n",
    "\n",
    "\n",
    "                #EXPERIENCE BUFFER\n",
    "                agent.update_replay_memory((current_state, action, reward, current_state, tot))\n",
    "                agent.train()\n",
    "                agent.tensorboard.update_stats(reward_avg=average_reward,\n",
    "                                            reward_min=min_reward,\n",
    "                                            reward_max=max_reward,\n",
    "                                            total_avg=avg_total,\n",
    "                                            total_max=max_total,\n",
    "                                            total_min=min_total)\n",
    "\n",
    "        print(epsilon)\n",
    "        if epsilon > MIN_EPSILON:\n",
    "            epsilon *= EPSILON_DECAY\n",
    "            epsilon = max(MIN_EPSILON, epsilon)\n",
    "except KeyboardInterrupt:\n",
    "    agent.model.save_weights('saved_weights/' + MODEL_NAME + 'primary')\n",
    "    agent.target_model.save_weights('saved_weights/' + MODEL_NAME + 'target')\n",
    "    with open('saved_weights/step_' + MODEL_NAME + '.txt', 'wb') as file:\n",
    "        pickle.dump([agent.tensorboard.step, episode, agent.replay_memory], file)\n",
    "    with open('saved_weights/total_' + MODEL_NAME + '.txt', 'wb') as file:\n",
    "        pickle.dump(total_memory, file)\n",
    "    with open('saved_weights/reward_' + MODEL_NAME + '.txt', 'wb') as file:\n",
    "        pickle.dump(total_reward, file)\n",
    "    print(f\"EXITED WITH {episode}th episode\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
